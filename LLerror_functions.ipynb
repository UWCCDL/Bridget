{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmXhk3y8XWNY",
        "outputId": "1bf2cb86-1d2e-4814-bc13-67ed928f00ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCH4OkskKt1g"
      },
      "outputs": [],
      "source": [
        "!which conda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kdjAJJHWw_v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "import pandas as pd\n",
        "import math\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "def activation(traces, time, decay):\n",
        "    \"\"\"Computes the activation of a memory given its history of retrievals\"\"\"\n",
        "    ftraces = [x for x in traces if x < time]\n",
        "    decay = max(0, decay)  # Allows no positive decay rates in equation\n",
        "    decay - min(decay, 5)\n",
        "    times = time - np.array(ftraces)\n",
        "    odds = times ** -decay\n",
        "    return np.log(np.sum(odds))\n",
        "\n",
        "\n",
        "def boltzmann(options, values, temperature):\n",
        "    \"\"\"Returns a Boltzmann distribution of the probabilities of each option\"\"\"\n",
        "    temperature = max(temperature, 0.01)\n",
        "    vals = np.array(values)/temperature\n",
        "    # bvals = np.exp(vals)/np.sum(np.exp(vals))\n",
        "    bvals = np.exp(vals - np.max(vals)) / np.exp(vals - np.max(vals)).sum()\n",
        "    return dict(zip(options, bvals))\n",
        "\n",
        "\n",
        "def responsetime(activation, ter, F=1, f=1):\n",
        "    return ter + F * np.exp(-f * activation)\n",
        "\n",
        "\n",
        "def rtProb(rt, activation, s, ter):\n",
        "    \"\"\"Takes one parameter for noise, s, and outputs a probability\n",
        "    distribution for response times\"\"\"\n",
        "    noise = np.linspace(-2, 2)\n",
        "    dist = sp.stats.logistic(0, ((math.pi**2)*s)/3)\n",
        "    rts = [responsetime((activation - x), ter) for x in noise]\n",
        "    prob = dist.pdf(noise)\n",
        "    rtprob = {rts[i]: prob[i]for i in range(len(noise))}\n",
        "    val = min(rtprob.keys(), key=lambda x: abs(x - (rt/1000)))\n",
        "    return rtprob[val]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LLelabRT(alldata, ppt, decay, temp, ter, mas=1.6):\n",
        "    data = alldata[alldata.participant == ppt]\n",
        "    # create a list of error items\n",
        "    errors = data[data.condition == 1].cue.tolist()\n",
        "    # create a list of study items\n",
        "    study = data[data.condition == 2].cue.tolist()\n",
        "    pos = 1\n",
        "    present = study[:]\n",
        "    for word in errors:\n",
        "        present.insert(pos, word)\n",
        "        pos += 2\n",
        "    # Create dict with word pairs\n",
        "    pairs = {}\n",
        "    for cue, target in zip(data.cue, data.target):\n",
        "        pairs[cue] = target\n",
        "    # also create a dict with errors\n",
        "    errorResp = dict()\n",
        "    for cue, response in zip(data.cue, data.study_response):\n",
        "        if isinstance(response, str):\n",
        "            errorResp[cue] = response\n",
        "\n",
        "    # model learning phase, encode a single trace for each item:\n",
        "    DM = dict()\n",
        "    # for DM can we make a dictionary of dictionaries where big keys are cues,\n",
        "    # values are dictionary of target/possible responses and their activation\n",
        "    time = 0\n",
        "    for cue in present:\n",
        "        littleDM = {}\n",
        "        study_responses = alldata[alldata.cue == cue]['study_response']\n",
        "        test_responses = alldata[alldata.cue == cue]['test_response']\n",
        "        # make a set of all reponses given to a certain cue to be\n",
        "        # \"vocab for that cue\"\n",
        "        for response in set(pd.concat([study_responses, test_responses])):\n",
        "            if isinstance(response, str):\n",
        "                littleDM[response] = [0.001]\n",
        "            # add retrieval of error for error items\n",
        "            if cue in errorResp.keys():\n",
        "                error = errorResp[cue]\n",
        "                time += 5\n",
        "                littleDM[error] = [0.001, time]\n",
        "                # overwrite smaller activ of correct target to show\n",
        "                # task learning\n",
        "                time += 5\n",
        "                littleDM[pairs[cue]] = [0.001, time]\n",
        "            else:\n",
        "                time += 10\n",
        "                littleDM[pairs[cue]] = [0.001, time]\n",
        "            DM[cue] = littleDM\n",
        "    time += 300  # time for distractor phase\n",
        "\n",
        "    # model testing phase\n",
        "    LL = 0\n",
        "    for condition, cue, target, \\\n",
        "        response, rt, feedback in zip(data.condition,\n",
        "                                      data.cue,\n",
        "                                      data.target,\n",
        "                                      data.test_response,\n",
        "                                      data.test_rt,\n",
        "                                      data.correct):\n",
        "        # Calculate log likelihood of response- possible options are 19 random\n",
        "        # integers or correct associate\n",
        "        options = DM[cue].keys()\n",
        "        # create spreading activation additional error component given size of\n",
        "        # cue's dec mem\n",
        "        cueMem = len(DM[cue])\n",
        "        add = (mas - np.log((cueMem + 1)/2)) - (mas - np.log((cueMem + 1)/1))\n",
        "        # if error condition, add spreading activation\n",
        "        values = [(activation(DM[cue][opt], time, decay) + add) if\n",
        "                  condition == 1 else activation(DM[cue][opt], time, decay)\n",
        "                  for opt in options]\n",
        "\n",
        "        # Set default value to be a random item in the dict:\n",
        "        prob = boltzmann(options, values, temp)[response]\n",
        "\n",
        "        # now calculate response times:\n",
        "        if condition == 1:\n",
        "            resp_activation = activation(DM[cue][response], time, decay) + add\n",
        "        else:\n",
        "            resp_activation = activation(DM[cue][response], time, decay)\n",
        "\n",
        "        prob_rt = rtProb(rt, resp_activation, temp, ter)\n",
        "\n",
        "        # Sum up the LLs\n",
        "        LL += (np.log(max(prob, 10e-10)) + np.log(max(prob_rt, 10e-10)))\n",
        "\n",
        "        # add time taken to responde\n",
        "        time += rt/1000\n",
        "    return LL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def rtProb2(rt, resp_activation, error_activation, condition, s, ter):\n",
        "    \"\"\"Takes one parameter for noise, s, and outputs a probability\n",
        "    distribution for response times\"\"\"\n",
        "    noise = np.linspace(-2, 2)\n",
        "    dist = sp.stats.logistic(0, ((math.pi**2)*s)/3)\n",
        "    if condition == 1:\n",
        "        rts = [(responsetime((resp_activation - x), ter) +\n",
        "                responsetime((error_activation - x), ter)) for x in noise]\n",
        "    else:\n",
        "        rts = [responsetime((resp_activation - x), ter) for x in noise]\n",
        "    prob = dist.pdf(noise)\n",
        "    rtprob = {rts[i]: prob[i]for i in range(len(noise))}\n",
        "    val = min(rtprob.keys(), key=lambda x: abs(x - (rt/1000)))\n",
        "    return rtprob[val]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LLmedRT(alldata, ppt, decay, temp, ter):\n",
        "    \"\"\"For each trial, calculate the probability of that response,\n",
        "    sum the log likelihoods, and update the values\"\"\"\n",
        "    data = alldata[alldata.participant == ppt]\n",
        "    # create a list of error items\n",
        "    errors = data[data.condition == 1].cue.tolist()\n",
        "    # create a list of study items\n",
        "    study = data[data.condition == 2].cue.tolist()\n",
        "    pos = 1\n",
        "    present = study[:]\n",
        "    for word in errors:\n",
        "        present.insert(pos, word)\n",
        "        pos += 2\n",
        "\n",
        "    # Create dict with word pairs\n",
        "    pairs = {}\n",
        "    for cue, target in zip(data.cue, data.target):\n",
        "        pairs[cue] = target\n",
        "    # also create a dict with errors\n",
        "    errorResp = dict()\n",
        "    for cue, response in zip(data.cue, data.study_response):\n",
        "        if isinstance(response, str):\n",
        "            errorResp[cue] = response\n",
        "\n",
        "    # model learning phase, encode a single trace for each item\n",
        "    DM = dict()\n",
        "    # for DM can we make a dictionary of dictionaries where big keys are cues,\n",
        "    # values are dictionary of target/possible responses and their activation\n",
        "    time = 0\n",
        "    for cue in present:\n",
        "        littleDM = {}\n",
        "        study_responses = alldata[alldata.cue == cue]['study_response']\n",
        "        test_responses = alldata[alldata.cue == cue]['test_response']\n",
        "        # make a set of all reponses given to a certain cue to be\n",
        "        # \"vocab for that cue\"\n",
        "        for response in set(pd.concat([study_responses, test_responses])):\n",
        "            if isinstance(response, str):\n",
        "                littleDM[response] = [0.001]\n",
        "        # add retrieval of error for error items\n",
        "        if cue in errorResp.keys():\n",
        "            error = errorResp[cue]\n",
        "            time += 5\n",
        "            littleDM[error] = [0.001, time]\n",
        "            # overwrite smaller activ of correct target to show task learning\n",
        "            time += 5\n",
        "            littleDM[pairs[cue]] = [0.001, time]\n",
        "        else:\n",
        "            time += 10\n",
        "            littleDM[pairs[cue]] = [0.001, time]\n",
        "        DM[cue] = littleDM\n",
        "    time += 300  # time for distractor phase\n",
        "\n",
        "    # model testing phase\n",
        "    LL = 0\n",
        "\n",
        "    for condition, cue, target, \\\n",
        "        response, rt, feedback in zip(data.condition,\n",
        "                                      data.cue,\n",
        "                                      data.target,\n",
        "                                      data.test_response,\n",
        "                                      data.test_rt,\n",
        "                                      data.correct):\n",
        "        # Calculate log likelihood of response-\n",
        "        # possible options are 19 random integers\n",
        "        # or correct associate\n",
        "        options = DM[cue].keys()\n",
        "\n",
        "        # calculate probability of retrieving given response\n",
        "        values = [activation(DM[cue][opt], time, decay) for opt in options]\n",
        "        prob1 = boltzmann(options, values, temp)[response]\n",
        "\n",
        "        # probability of retrieving error memory\n",
        "        if condition == 1:\n",
        "            error = errorResp[cue]\n",
        "            prob2 = boltzmann(options, values, temp)[error]\n",
        "        else:\n",
        "            prob2 = 0\n",
        "\n",
        "        # add response times calculations\n",
        "        # probability of given response time with\n",
        "        respAct = activation(DM[cue][response], time, decay)\n",
        "        if condition == 1:\n",
        "            error = errorResp[cue]\n",
        "            errorAct = activation(DM[cue][error], time, decay)\n",
        "            prob_rt = rtProb2(rt, respAct, errorAct, condition, temp, ter)\n",
        "        else:\n",
        "            errorAct = 0\n",
        "            prob_rt = rtProb2(rt, respAct, errorAct, condition, temp, ter)\n",
        "\n",
        "        # Sum up the LLs\n",
        "        LL += (np.log(max(prob1 + prob2, 10e-10)) +\n",
        "               np.log(max(prob_rt, 10e-10)))\n",
        "\n",
        "        # add time taken to responde\n",
        "        time += rt/1000\n",
        "\n",
        "    return LL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def vLLelab(array, data, ppt):\n",
        "    \"\"\"Vector function of procedural log-likelihood\"\"\"\n",
        "    decay, temp, ter = array\n",
        "    return -1 * LLelabRT(data, ppt, decay, temp, ter)\n",
        "\n",
        "\n",
        "def vLLmed(array, data, ppt):\n",
        "    \"\"\"Vector function of procedural log-likelihood\"\"\"\n",
        "    decay, temp, ter = array\n",
        "    return -1 * LLmedRT(data, ppt, decay, temp, ter)\n",
        "\n",
        "\n",
        "def ll_participant(alldata, ppt, LL_data):\n",
        "    data = alldata[alldata.participant == ppt]\n",
        "    print(data)\n",
        "    edecay, etemp, eter = opt.minimize(vLLelab, x0=[0.5, 1, 1],\n",
        "                                       args=(data, ppt),\n",
        "                                       method=\"Powell\",\n",
        "                                       bounds=[[0.01, 2], [0, 2], [0.1, 2]]).x\n",
        "    llelab = LLelabRT(alldata, ppt, edecay, etemp, eter)\n",
        "\n",
        "    mdecay, mtemp, mter = opt.minimize(vLLmed, x0=[0.5, 1, 1],\n",
        "                                       args=(data, ppt),\n",
        "                                       method=\"Powell\",\n",
        "                                       bounds=[[0.01, 2], [0, 2], [0.1, 2]]).x\n",
        "    llmed = LLmedRT(alldata, ppt, mdecay, mtemp, mter)\n",
        "\n",
        "    best = \"Mediator\"\n",
        "    if llelab > llmed:\n",
        "        best = \"Elaborative\"\n",
        "\n",
        "    diff = llmed - llelab\n",
        "\n",
        "    row = [ppt, edecay, etemp, eter, llelab,\n",
        "           mdecay, mtemp, mter, llmed, best, diff]\n",
        "\n",
        "    LL_data = pd.concat([LL_data, pd.DataFrame([row],\n",
        "                                               columns=LL_data.columns)],\n",
        "                        ignore_index=True)\n",
        "\n",
        "    return row, LL_data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
